{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed9359e",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [General information](#section_1_1)\n",
    "* [Online presence](#section_2_1)\n",
    "* [Review](#section_3_1)\n",
    "* [Reproduction](#section_4_1)\n",
    "* [Tests and results](#section_5_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d03ba",
   "metadata": {},
   "source": [
    "### General information  <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75bb81e",
   "metadata": {},
   "source": [
    "<b>Article name</b>: You Only Look Once: Unified, Real-Time Object Detection </br>\n",
    "<b>Subjects</b>: Computer Vision and Pattern Recognition </br>\n",
    "<b>Authors</b>: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf76ba5",
   "metadata": {},
   "source": [
    "### Online presence  <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb804a0",
   "metadata": {},
   "source": [
    "__[arxiv.org](https://arxiv.org/abs/1506.02640)__ </br>\n",
    "__[springer.com](https://www.springer.com/journal/11263)__ </br>\n",
    "__[ieeexplore.ieee.org](https://ieeexplore.ieee.org/xpl/conhome/1000147/all-proceedings)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e2a71",
   "metadata": {},
   "source": [
    "### Review  <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819df655",
   "metadata": {},
   "source": [
    "<b>Abstract</b>:</br>\n",
    "This scientific paper addresses the problem of real-time object detection, suggesting an innovative model named YOLO (You Only Look Once). Unlike traditional object detection methods that require multiple stages such as region proposals and subsequent classification, YOLO conducts object detection as a single regression problem to spatially separated bounding boxes and associated class probabilities. </br></br>\n",
    "<b>Key Points</b>:\n",
    " - <i>Unified Detection</i>: YOLO introduces a new architecture that simultaneously predicts bounding box coordinates and class probabilities for all objects in a single forward pass. This outperforms the traditional approaches.\n",
    " - <i>Real-Time Performance</i>: The new technique allows YOLO to achieve real-time performance, making it suitable for applications requiring fast and efficient object detection.\n",
    " - <i>End-to-End Model</i>: The model is trained end-to-end, optimizing the detection performance directly without relying on pre-trained models for different tasks.\n",
    " - <i>Grid-based Prediction</i>: The image is divided into a grid, and each grid cell predicts bounding boxes and class probabilities. This grid-based approach aids in capturing objects of different scales and locations.\n",
    " - <i>Bounding Box Encoding</i>: YOLO uses a novel encoding scheme for bounding boxes, allowing the model to predict bounding box dimensions and offsets directly, contributing to the simplicity and efficiency of the architecture.\n",
    " \n",
    "<b>Methodology</b>:</br>\n",
    " - <i>Network Architecture</i>: The YOLO architecture consists of 24 convolutional layers followed by 2 fully connected layers. The final layer predicts bounding box coordinates and class probabilities.\n",
    " - <i>Loss Function</i>: YOLO employs a customized loss function that penalizes localization errors and classification errors. This loss function contributes to the end-to-end training of the model.\n",
    " - <i>Non-Maximum Suppression</i>: Post-processing involves non-maximum suppression to eliminate duplicate detections and improve the final output.\n",
    " \n",
    "<b>Evaluation</b>:</br>\n",
    " - <i>Datasets</i>: The authors evaluate YOLO on standard object detection datasets, including VOC and COCO, demonstrating competitive performance compared to existing methods.\n",
    " - <i>Real-Time Performance</i>: YOLO achieves impressive real-time performance, with the ability to process images at 45 frames per second.\n",
    " \n",
    "<b>Pros and Cons</b>:</br>\n",
    " - Pros:\n",
    "    - Unified architecture simplifies the object detection pipeline.\n",
    "    - Real-time performance suitable for applications like video analysis.\n",
    "    - End-to-end training facilitates optimization for detection tasks.\n",
    " - Cons:\n",
    "    - YOLO might struggle with small objects due to the fixed grid-based approach.\n",
    "    - Precision might be sacrificed for real-time speed in certain scenarios.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1583842",
   "metadata": {},
   "source": [
    "### Reproduction  <a class=\"anchor\" id=\"section_4_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a24e67",
   "metadata": {},
   "source": [
    "Notes: I personally managed to reproduce the results on Ubuntu Machine with the following kernel details: </br>\n",
    "<i>uname -a</i></br>\n",
    "Linux leonidg 5.4.0-150-generic #167~18.04.1-Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e55c7a",
   "metadata": {},
   "source": [
    "The reproduction requires downloading a lot of data (nearly 300 MB). That is why it is not included in this notebook. Considering the limits I can provide only the results of my tests and output of a console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee40cfe",
   "metadata": {},
   "source": [
    "<b>Setup process</b>:</br>\n",
    "<i>Step 1)</i> - update the packages and install the required ones:\n",
    " - sudo apt-get update\n",
    " - sudo apt-get install build-essential cmake git libopencv-dev libgtk-3-dev\n",
    "\n",
    "<i>Step 2)</i> - clone the repo that contains YOLO implementation:\n",
    " - git clone https://github.com/AlexeyAB/darknet.git\n",
    "\n",
    "<i>Step 3)</i> - make it executable:\n",
    " - cd darknet\n",
    " - make\n",
    "\n",
    "<i>Step 4)</i> - download pre-trained YOLO weights file:\n",
    " - wget https://pjreddie.com/media/files/yolov3.weights\n",
    "\n",
    "<i>Step 5)</i> - download default images or other from the Net\n",
    "\n",
    "<i>Step 6)</i> - run YOLO on the images\n",
    " - ./darknet detect cfg/yolov3.cfg yolov3.weights image_file_name.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a8ca0",
   "metadata": {},
   "source": [
    "<b>Example run</b>:</br>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7daf1909",
   "metadata": {},
   "source": [
    "$ ./darknet detect cfg/yolov3.cfg yolov3.weights dog_cat.jpeg\n",
    " GPU isn't used \n",
    " OpenCV isn't used - data augmentation will be slow \n",
    "mini_batch = 1, batch = 1, time_steps = 1, train = 0 \n",
    "   layer   filters  size/strd(dil)      input                output\n",
    "   0 conv     32       3 x 3/ 1    416 x 416 x   3 ->  416 x 416 x  32 0.299 BF\n",
    "   1 conv     64       3 x 3/ 2    416 x 416 x  32 ->  208 x 208 x  64 1.595 BF\n",
    "   2 conv     32       1 x 1/ 1    208 x 208 x  64 ->  208 x 208 x  32 0.177 BF\n",
    "   3 conv     64       3 x 3/ 1    208 x 208 x  32 ->  208 x 208 x  64 1.595 BF\n",
    "   4 Shortcut Layer: 1,  wt = 0, wn = 0, outputs: 208 x 208 x  64 0.003 BF\n",
    "   5 conv    128       3 x 3/ 2    208 x 208 x  64 ->  104 x 104 x 128 1.595 BF\n",
    "   6 conv     64       1 x 1/ 1    104 x 104 x 128 ->  104 x 104 x  64 0.177 BF\n",
    "   7 conv    128       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x 128 1.595 BF\n",
    "   8 Shortcut Layer: 5,  wt = 0, wn = 0, outputs: 104 x 104 x 128 0.001 BF\n",
    "   9 conv     64       1 x 1/ 1    104 x 104 x 128 ->  104 x 104 x  64 0.177 BF\n",
    "  10 conv    128       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x 128 1.595 BF\n",
    "  11 Shortcut Layer: 8,  wt = 0, wn = 0, outputs: 104 x 104 x 128 0.001 BF\n",
    "  12 conv    256       3 x 3/ 2    104 x 104 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  13 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    "  14 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  15 Shortcut Layer: 12,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
    "  16 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    "  17 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  18 Shortcut Layer: 15,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
    "  19 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    "  20 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  21 Shortcut Layer: 18,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
    "  22 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    "  23 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  24 Shortcut Layer: 21,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
    "  25 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    "  26 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  27 Shortcut Layer: 24,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
    "  28 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    "  29 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  30 Shortcut Layer: 27,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
    "  31 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    "  32 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  33 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
    "  34 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    "  35 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    "  36 Shortcut Layer: 33,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
    "  37 conv    512       3 x 3/ 2     52 x  52 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  38 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  39 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  40 Shortcut Layer: 37,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
    "  41 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  42 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  43 Shortcut Layer: 40,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
    "  44 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  45 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  46 Shortcut Layer: 43,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
    "  47 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  48 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  49 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
    "  50 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  51 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  52 Shortcut Layer: 49,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
    "  53 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  54 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  55 Shortcut Layer: 52,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
    "  56 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  57 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  58 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
    "  59 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  60 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  61 Shortcut Layer: 58,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
    "  62 conv   1024       3 x 3/ 2     26 x  26 x 512 ->   13 x  13 x1024 1.595 BF\n",
    "  63 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
    "  64 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
    "  65 Shortcut Layer: 62,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
    "  66 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
    "  67 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
    "  68 Shortcut Layer: 65,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
    "  69 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
    "  70 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
    "  71 Shortcut Layer: 68,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
    "  72 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
    "  73 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
    "  74 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
    "  75 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
    "  76 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
    "  77 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
    "  78 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
    "  79 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
    "  80 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
    "  81 conv    255       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 255 0.088 BF\n",
    "  82 yolo\n",
    "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
    "  83 route  79 \t\t                           ->   13 x  13 x 512 \n",
    "  84 conv    256       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x 256 0.044 BF\n",
    "  85 upsample                 2x    13 x  13 x 256 ->   26 x  26 x 256\n",
    "  86 route  85 61 \t                           ->   26 x  26 x 768 \n",
    "  87 conv    256       1 x 1/ 1     26 x  26 x 768 ->   26 x  26 x 256 0.266 BF\n",
    "  88 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  89 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  90 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  91 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
    "  92 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
    "  93 conv    255       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 255 0.177 BF\n",
    "  94 yolo\n",
    "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
    "  95 route  91 \t\t                           ->   26 x  26 x 256 \n",
    "  96 conv    128       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x 128 0.044 BF\n",
    "  97 upsample                 2x    26 x  26 x 128 ->   52 x  52 x 128\n",
    "  98 route  97 36 \t                           ->   52 x  52 x 384 \n",
    "  99 conv    128       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x 128 0.266 BF\n",
    " 100 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    " 101 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    " 102 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    " 103 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
    " 104 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
    " 105 conv    255       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 255 0.353 BF\n",
    " 106 yolo\n",
    "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
    "Total BFLOPS 65.879 \n",
    "avg_outputs = 532444 \n",
    "Loading weights from yolov3.weights...\n",
    " seen 64, trained: 32013 K-images (500 Kilo-batches_64) \n",
    "Done! Loaded 107 layers from weights-file \n",
    " Detection layer: 82 - type = 28 \n",
    " Detection layer: 94 - type = 28 \n",
    " Detection layer: 106 - type = 28 \n",
    "dog_cat.jpeg: Predicted in 10395.108000 milli-seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933105e",
   "metadata": {},
   "source": [
    "### Tests and results <a class=\"anchor\" id=\"section_5_1\"></a>\n",
    "During the tests, I have used both images included in the git repo and other downloaded from the Net :</br></br>\n",
    "<i>Large images with easily detectable objects </i>:</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207ca1d",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"predictions_dog.jpg\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"predictions_random.jpg\" style=\"width: 300px;\"/> </td>\n",
    "<td> <img src=\"predictions_bird.jpg\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf1b27",
   "metadata": {},
   "source": [
    "<i>Small images in which the objects may overlap </i>:</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb995dfd",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"predictions_3dogs.jpg\" style=\"width: 350px;\"/> </td>\n",
    "<td> <img src=\"predictions_small.jpg\" style=\"width: 350px;\"/> </td>\n",
    "</tr></table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
